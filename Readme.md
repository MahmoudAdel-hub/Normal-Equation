# Features 

<ol>
<li> less than 10000 >> Normal Equation is faster than Gradient Descent </li>
<li> more than 10000 >> Gradient Descent is faster than Normal Equation </li>
</ol>

# Normal Equation 
 
<ol>
<li> Without Choosing Learning Rate(Alpha) </li> 
<li> Features Less Than 1000 ( Works With Small Number Of Features Very Well) </li> 
<li> Works Without Using Features Scaling </li>
</ol>

# Gradient Descent 

<ol>
<li> Choosing Learning Rate(Alpha) </li> 
<li> Features More Than 10000 ( Works With Big Number Of Features Very Well ) </li> 
<li> Works With Using Features Scaling </li> 
</ol>

<mark> Why Gradient Descent Works Very Well With Big Number Of Features than Normal Equation ? </mark>
- Normal Equation Complexity N^3 
- Gradient Descent Complexity N^2
